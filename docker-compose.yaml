x-base-config: &base-config
  restart: always
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"
  networks:
    - vllm-network

services:


  # vllm:
  #   image: vllm/vllm-openai:v0.10.2
  #   container_name: vllm
  #   ipc: host
  #   ports:
  #     - "9009:9009"
  #     #   - "8000:8000" # 8000 IS UNUSED
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   env_file:
  #     - .env
  #   environment:
  #     HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
  #     TORCH_CUDA_ARCH_LIST: 7.5
  #     VLLM_USE_V1: 0
  #   command: --model Qwen/Qwen3-1.7B --served-model-name qwen3-1.7b --enforce-eager --max-model-len 3k --host 0.0.0.0 --port 9009 --swap-space 0
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   <<: *base-config

  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:v1.75.5-stable
    ports:
      - "8001:4000"
    volumes:
      - ./litellm_config/litellm_config.yaml:/app/config.yaml # Mount the local configuration file
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    env_file:
      - .env
    environment:
      LITELLM_MASTER_KEY: $LITELLM_MASTER_KEY
      NO_DOCS: ${LITELLM_NO_DOCS:-true}
      UI_USERNAME: $LITELLM_UI_USERNAME
      UI_PASSWORD: $LITELLM_UI_KEY
      DATABASE_URL: $LITELLM_DB_URI
    depends_on:
      # - vllm
      - postgredb
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    <<: *base-config

  vllm_logger:
    container_name: vllm_logger
    build:
      context: ./vllm_logger
      dockerfile: Dockerfile
    ports:
      - "8008:8000"
    env_file:
      - .env
    depends_on:
      - postgredb
    volumes:
      - ./vllm_logger/ClickhouseDB/:/app/ClickhouseDB
      - ./vllm_logger/logger.py:/app/logger.py
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    <<: *base-config

  backend:
    container_name: backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      - postgredb
      - clickhouse
    volumes:
      - ./backend/app:/app/app
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    <<: *base-config

  postgredb:
    image: pgvector/pgvector:pg18
    container_name: postgredb
    # environment:
    #   - DB_HOST=localhost
    env_file:
      - .env
    volumes:
      - postgres-data:/var/lib/postgresql
      - ./postgres_config/init.sh:/docker-entrypoint-initdb.d/init.sh
    restart: always
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 3s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        limits:
          memory: 3g
        reservations:
          memory: 2g
    <<: *base-config

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    environment:
      CLICKHOUSE_DB: ${CLICKHOUSE_DB_VLLM_LOGGER}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ./clickhouse_config/init_database.sql:/docker-entrypoint-initdb.d/init_database.sql
      - ./clickhouse_config/logging.xml:/etc/clickhouse-server/logging.xml
    deploy:
      resources:
        limits:
          memory: 2g
        reservations:
          memory: 1g
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 5s
      timeout: 5s
      retries: 5
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    <<: *base-config

  minio:
    image: minio/minio:latest
    container_name: mlflow-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    # ports:
    #   - "9000:9000"
    #   - "9001:9001"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-f",
          "http://localhost:${MINIO_PORT}/minio/health/live",
        ]
      interval: 5s
      timeout: 3s
      retries: 20
    <<: *base-config

  create-bucket:
    image: minio/mc:latest
    container_name: mlflow-create-bucket
    depends_on:
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - -c
      - |
        mc alias set myminio "http://${MINIO_HOST}:${MINIO_PORT}" \
          "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}"
        mc mb --ignore-existing "myminio/${MLFLOW_MINIO_BUCKET:-mlflow}"
    restart: "no"
    <<: *base-config

  mlflow:
    image: ghcr.io/mlflow/mlflow:${MLFLOW_VERSION}
    container_name: mlflow-server
    depends_on:
      postgredb:
        condition: service_healthy
      minio:
        condition: service_healthy
      create-bucket:
        condition: service_completed_successfully
    environment:
      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_STORE_URI}

      # S3/MinIO settings
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      MLFLOW_ARTIFACTS_DESTINATION: ${MLFLOW_ARTIFACTS_DESTINATION}
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      MLFLOW_S3_IGNORE_TLS: "true"

      # Server host/port
      MLFLOW_HOST: ${MLFLOW_HOST}
      MLFLOW_PORT: ${MLFLOW_PORT}

    command:
      - /bin/bash
      - -c
      - |
        pip install --no-cache-dir psycopg2-binary boto3
        mlflow server \
          --backend-store-uri "${MLFLOW_BACKEND_STORE_URI}" \
          --artifacts-destination "${MLFLOW_ARTIFACTS_DESTINATION}" \
          --host "${MLFLOW_HOST}" \
          --port "${MLFLOW_PORT}"
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:${MLFLOW_PORT}/health')",
        ]
      interval: 10s
      timeout: 5s
      retries: 30
    <<: *base-config

networks:
  vllm-network:
    driver: bridge

volumes:
  postgres-data:
    driver: local
  minio-data:
    driver: local
  clickhouse-data:
    driver: local
